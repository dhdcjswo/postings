# Multi-Armed Bandits#2 Thompson sampling(1)





## Bernoulli bandit

우리가 선택할 수 있는 대안이 K개 있다 $k \in \{1,2,..,K \}$. 카지노를 예를들면 K개의 슬롯머신이 있는 셈이다. 우리가 슬롯머신 하나를 골라 게임을 진행하면 '성공' 또는 '실패'의 보상을 받는다고 하자. 각 K개의 슬롯머신에서 '성공'의 확률은 $ 0 \le \theta_k \le 1$  이고 슬롯머신 K개의 성공확률의 집합은 $(\theta_1,..\theta_K)$ 이다. 하지만 우리는 성공확률을 모르는 상황이다. 그렇기에 실험을 통해서 찾아나가야 한다. 우리의 목표는 T의 기간동안 누적 성공횟수를 최대화하는 것이 목적이다.

웹사이트 광고를 예를들면, 광고의 종류가 여러개가 있고 어떤 광고를 보여주었을 때 사용자들이 클릭을 많이하는지, 다시말하면 사용자 대비 광고를 클릭한 사용자 수를 위에서 설명한 $\theta_k$  같은 것으로 생각할 수 있다. 

간단하게 접근하는 방법은 특정 기간동안 모든 광고에 대해 동일한 확률로 사용자들에게 보여주어 정보를 수집한후, 수집한 정보를 토대로 가장 높은 확률을 보인 광고를 선택하여 홈페이지 배너에 게시하는것이다. A/B 테스트와 같은 전략이다.

문제는 exploration과 exploitation 사이의 밸런스이다. 너무 짧은 기간동안 정보를 수집해 의사결정을 내리면 잘못된 의사결정을 내릴 수 있다. 반대로 정보수집 기간이 너무 길면, 변화를 빨리 적용시키지 못해 발생하는 손해가 클 수 있다. 적절한 밸런스를 맞추어야 하는데 그 **'적절한'** 밸런스는 실제로 모르거나 달성하기 어려운 경우가 많다.



![1](img/MAB2_1.PNG)

우리는 노드1 에서 출발하여 노드12 로 도착하는 가장 빠른 경로를 알고 싶다. 하지만 어떤 경로가 가장 빠른지 우리는 모르는 상황이다. Bernoulli-bandit의 경우, 새로운 슬롯머신이 하나 추가되면 모수가 단순히 하나가 늘어난다. 하지만 이 경우 노드 하나가 추가되면 늘어나는 경로는 기하급수이다. 계산의 측면에서 challenging하게 된다.



앞으로 설명할 **Thompson sampling**은 이러한 면에서 세련되고 유연한 알고리즘이다.알고리즘은 처음 등장한 것은 1933년인데 수렴에 대한 증명 등 실제로 검증이 되어 사용이 되기 시작한것은 그리 오래 되지 않았다(아마 2010년 경). 



## greedy decision

**'greedy'** 알고리즘은 단순하고 가장 일반적인 방법론이다. 앞서 설명한대로 1)과거데이터를 통해 모델을 평가하고 2) 최적의 모델을 사용하는 과정이다. supervised-learning과 같다. 데이터와 레이블 쌍 ($x_t$,$y_t$) 이 존재하고 이 데이터를 통해서 우리가 알고싶은 모수 $\theta_t$를 추정한다.

![2](img/MAB2_2.PNG)



슬롯머신의 예로 돌아가면 매 시간 t 다 각각의 슬롯머신의 성공 확률 $(\theta_1 ..\theta_k)$ 를 계산하고 가장 높은 $\theta_k$를 가지는 슬롯머신을 선택해 게임을 진행한다. 이 경우에는 보상이 성공:1 , 실패:0 인 경우 이기에 확률을 비교하여 선택하지만 엄밀히 말하면 **$\theta_k$ x 성공보상**  이 가장 높은 슬롯 머신을 택하는 것이다.(평균 보상)  성공보수가 1이라고 하면 성공횟수/시행횟수 를 통해서 해당 슬롯머신의 성공확률을 계산할것이다. 그 후 최적의 확률을 가지는 슬롯머신을 선택하고 게임을 진행하며 보상을 받고 이 정보를 다시 다음 t 에 사용하여 확률을 계산하는 식으로 진행된다.

예를 들어보자. 3가지의 action 선택지가 있다.(3개의 슬롯머신) 우리는 action1,action2 는 각 1000번, action3는 5번 탐험을 했다.(exploration) 그리고 누적된 성공보상은 각 600,400,2 이다. 이 경우에 우리는 action1과 action2에 대해서는 평균 기대 보상이 각 0.6, 0.4에 가까울 것이라는 강한 확신을 가질 수있다. 하지만 action3의 경우는 탐험 횟수가 부족해서 섣부르게 결정하지 못하는 높은 불확실성이 존재한다.

이러한 예 에서 greedy 알고리즘은 action1 을 선택할 거이다. 왜냐면 기대평균이 가장 높으니깐(0.6).  $\theta_2> \theta_1$ 이라고는 결정하기 힘들것이다. 왜냐면 600,400번의 경험이 있고 기대 평균이 좀처럼 바뀌지 않을 것이기 때문이다.문제는 $\theta_3 > \theta1$ 의 경우다. 이게 실제와 같다면 문제가 되지 않지만 경험 횟수가 적어서 불확실성이 큰 상황에서 greedy 알고리즘은 항상 $\theta_1$ 만 택할 것이기 때문에 좀처럼 $\theta_3$ 대한 학습을 하기 힘들것이다. 

이를 보완하고자 나온게 **&epsilon; - greedy** 알고리즘이다. 