# Multi-Armed Bandits#1_intro



​	슬롯머신으로 가득한 카지노가 있다. 겜블러는 수많은 슬롯 머신중에서 하나를 선택하여 게임을 진행하려고 한다. 겜블러의 목적은 매 time-step 마다 하나의 슬롯머신을 선택해서 게임을 진행해 최대한 많은 돈을 따는 것이다.    단순히 이기는 것이 목적이 아니라 '기대 이익'의 관점에서 최고의 의사결정은 무엇일까.

문제는 exploration(탐험) 과 exploitation(이용) 이다. 아무런 정보가 없는 상태에서 무작정 하나의 슬롯머신을 선택해서 주야장천 한다면 더 나은 기대보상을 주는 슬롯머신이 있음에도 이를  놓칠 수 있다. exploration이 충분치 못한 경우다.  반대로 exploration에 너무 많은 비용을 투자하면 최적의 슬롯은 찾을 수 있더라도 이미 소비한 비용이 많아서 이익을 최대화 하지 못하는 경우도 생긴다. 이를 **Multi-Armed bandit problem** 이라고 한다.







## definition

**Multi-Armed Bandit(MAB)** 은 다양한 행동들 중에서 최대의 기대이익을 가져다 주는 행동을 선택하는 문제를 의미한다. 아무런 정보가 없는 상황에서 적절한 exploration으로 정보를 얻고, exploitation을 통해서 정보를 활용하여 기대이익을 극대화 해야한다. 이 때의 행동의 선택지에 따라 'n-armed bandit' 으로 구분할 수 있다.





## A/B test

**MAB** 는 흔히 A/B 테스트에 세련된 형태라고 이야기한다. A/B 테스트의 경우 선택지 두가지를 정해진 기간에 두 그룹에게 보여준후 전환율의 차이를 통해서 더 나은 '안'을 선택하는 방법이다.  A/B 테스트에서 더 나은 '안'을 선택하기위해서 A,B안을 두 그룹에게 보여주는 것은 MAB의 관점에서는 최적의 슬롯을 고르기 위해 탐색하는 exploration과 같다. 하지만 A/B테스트에서는 이 단계가 의사결정을 위한 데이터를 수집하는 단계임과 동시에 새로운 안을 바로 적용시키지 못해 손해가 발생하는 단계라고 볼수도 있다. MAB의 경우 매 시간 스텝마다 알고리즘을 발전시켜 나가기 때문에 exploration과 exploitation을 동시에 진행하는것과 같고 생기지 않았을 손해도 줄일 수 있다. 





## solution



**1. Epsilon-Greedy**

이 전략은 exploration과 exploitation 사이의 균형을 유지하는 전략이다. 매 시간 스텝마다 &epsilon; 확률로 무작위로 슬롯을 하나 골라서 게임을 진행하고 1-&epsilon;  확률로 기존의 경험했던 슬롯 머신들중 가장 기대이익이 높았던 슬롯을 고르는 전략이다. &epsilon; (0 <  &epsilon; < 1) 의 값을 조절함으로써 exploration에 비중을 많이 둘건지 exploitation에 비중을 둘것인지 정할 수 있다. 만약 &epsilon; =0.05  라 정한다면, 이 알고리즘은 0.05확률만큼 새로운 슬롯을 고르는 exploration 행동을 취할것이고 0.95의 확률만큼 원래 알던 슬롯중 최고의 슬롯에서 게임을 할 것이다. 경험적인 전략이고 효과적인 전략이지만 경험을 통한 최적의 슬롯이 실제 최적이 아닌 sub-optimal 슬롯일수 있고 그렇기에 local-maximum에 빠질수 있는 가능성이 있다. 





**2. Upper Confidence Bound(UCB)**

어떤 슬롯이 다른 슬롯보다 낫다고 어떻게 정의 내릴수 있을까.기본적으로 슬롯머신은 확률에 의해서 보상이 주어진다. 경험에 근거한 기대이익이 크다고해서 해당슬롯이 항상 최적의 보상을 해주는 것은 아니다. 확률변수가 가지는 분포에서 샘플링된 보상이 주어지는 것일뿐 고정된 보상이 아니기 때문이다. 따라서 내가 받은 보상이 확률분포에서의 어느 위치에 있는가를 고려해야한다. 가령 횟수가 적은 경우는 그값의 편차가 크기 때문에 실제 확률변수의 기대값보다 과대/과소 평가할 수도 있다. 따라서 더 좋은 슬롯이라고 평가하기 위해서 경험적인 데이터를 통한 근거가 있어야 한다.  UCB는 이름 그대로 해당 기대값이 가지는 신뢰구간의 upper 경계를 구해서 슬롯간의 상대적인 비교를 하는 알고리즘이다. 





**3. Thompson sampler(Posterior sampler)**

베이지안 에 기반한 Thompson 알고리즘은 ,  겜블러가 선택하는 행동(a)과 슬롯이 가지는 모수(&theta;) 를 통해서 보상(r)을 받는 결과를 확률에 기반하여 베이즈 정리에 적용시켜 posterior 사후 분포를 구해서 문제를 푸는 알고리즘이다. 베이지안의 가지는 일반적인 장점들을 가지고 있고 그 성능도 좋아서 최근에 많이 사용 되는 알고리즘이다.





큰 세가지 알고리즘에 대해서 간략하게만 서술해보았다. 각 알고리즘의 자세한 내용들에 대해서는 추가 포스팅을 통해서 알아보도록 하자.







## reference

http://sanghyukchun.github.io/96/

https://banditalgs.com/2016/09/18/the-upper-confidence-bound-algorithm/

https://sumniya.tistory.com/9

https://towardsdatascience.com/solving-multiarmed-bandits-a-comparison-of-epsilon-greedy-and-thompson-sampling-d97167ca9a50